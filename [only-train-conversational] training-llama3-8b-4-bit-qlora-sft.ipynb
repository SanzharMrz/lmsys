{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformers==4.42.4\n",
    "# bitsandbytes==0.43.1\n",
    "# accelerate==0.32.1\n",
    "# peft==0.11.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/llama-sft/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import copy\n",
    "from dataclasses import dataclass\n",
    "import re\n",
    "import json\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import Dataset\n",
    "from scipy.special import softmax\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from transformers import (\n",
    "    BitsAndBytesConfig,\n",
    "    LlamaPreTrainedModel,\n",
    "    LlamaModel,\n",
    "    AutoTokenizer,\n",
    "    PreTrainedTokenizerBase, \n",
    "    EvalPrediction,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorForSeq2Seq,\n",
    ")\n",
    "from transformers.modeling_outputs import CausalLMOutputWithPast\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, TaskType\n",
    "from sklearn.metrics import log_loss, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msolostringer\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/jupyter/.netrc\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jupyter/llama3-experiments/wandb/run-20240728_181001-ochveog4</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/solostringer/lmsys/runs/ochveog4' target=\"_blank\">sft convo all ultra feedback</a></strong> to <a href='https://wandb.ai/solostringer/lmsys' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/solostringer/lmsys' target=\"_blank\">https://wandb.ai/solostringer/lmsys</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/solostringer/lmsys/runs/ochveog4' target=\"_blank\">https://wandb.ai/solostringer/lmsys/runs/ochveog4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/solostringer/lmsys/runs/ochveog4?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7fb639566290>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.login(key=os.environ['WANDB_KEY'])\n",
    "\n",
    "wandb.init(\n",
    "    project=\"lmsys\",\n",
    "    name='sft convo all ultra feedback',\n",
    "    notes=\"same params, enlarged max length\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "TRAIN_CSV = \"data/train.csv\"\n",
    "model_path = \"unsloth/llama-3-8b-Instruct-bnb-4bit\"\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "target_columns = ['winner_model_a', 'winner_model_b', 'winner_tie']\n",
    "columns_to_vectorize = [\"prompt\", \"response_a\", \"response_b\"]\n",
    "\n",
    "train = pd.read_csv(TRAIN_CSV)\n",
    "\n",
    "train['label'] = train[target_columns].idxmax(axis=1) \n",
    "label_encoder = LabelEncoder()\n",
    "train['label'] = label_encoder.fit_transform(train['label'])\n",
    "train = train[columns_to_vectorize + ['label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizer and prepare dataset, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "tokenizer.add_eos_token = True\n",
    "tokenizer.padding_side = 'right'\n",
    "\n",
    "# Define label IDs\n",
    "LABEL_IDS = [tokenizer(i, add_special_tokens=False)[\"input_ids\"][0] for i in ['a', 'b', 'tie']]\n",
    "\n",
    "MAX_LENGTH = 1700  # Adjust based on your requirements\n",
    "\n",
    "def accumulate_and_truncate_conversation(prompts, responses, max_length, tokenizer, bot_name):\n",
    "    tokens = []\n",
    "    for prompt, response in zip(prompts, responses):\n",
    "        user_text = f'### User: \"{prompt}\"\\n\\n'\n",
    "        response_text = f'### Bot {bot_name} Response: \"{response}\"\\n\\n'\n",
    "        user_tokens = tokenizer(user_text, add_special_tokens=False)[\"input_ids\"]\n",
    "        response_tokens = tokenizer(response_text, add_special_tokens=False)[\"input_ids\"]\n",
    "        if len(tokens) + len(user_tokens) + len(response_tokens) > max_length:\n",
    "            # Truncate the response tokens to fit the remaining space\n",
    "            available_space = max_length - len(tokens) - len(user_tokens)\n",
    "            if available_space > 0:\n",
    "                response_tokens = response_tokens[:available_space]\n",
    "                tokens += user_tokens + response_tokens\n",
    "            break\n",
    "        tokens += user_tokens + response_tokens\n",
    "    return tokens\n",
    "\n",
    "def tokenize(example, tokenizer):\n",
    "    initial_prompts = eval(example['prompt'], {\"null\": \"\"})\n",
    "    initial_responses = eval(example['response_a'], {\"null\": \"\"})\n",
    "    follow_up_prompts = eval(example['prompt'], {\"null\": \"\"})\n",
    "    follow_up_responses = eval(example['response_b'], {\"null\": \"\"})\n",
    "    \n",
    "    # Add the separator for conversation with Bot A\n",
    "    conversation_a_separator = tokenizer('~~~~~~~~~~ CONVERSATION WITH BOT A ~~~~~~~~~~\\n\\n', add_special_tokens=False)[\"input_ids\"]\n",
    "    conversation_a = accumulate_and_truncate_conversation(initial_prompts, initial_responses, MAX_LENGTH // 2, tokenizer, 'A')\n",
    "    \n",
    "    # Add the separator for conversation with Bot B\n",
    "    conversation_b_separator = tokenizer('\\n\\n~~~~~~~~~~ CONVERSATION WITH BOT B ~~~~~~~~~~\\n\\n', add_special_tokens=False)[\"input_ids\"]\n",
    "    conversation_b = accumulate_and_truncate_conversation(follow_up_prompts, follow_up_responses, MAX_LENGTH // 2, tokenizer, 'B')\n",
    "    \n",
    "    # Add the final separator and the final question\n",
    "    final_separator = tokenizer('\\n\\n~~~~~~~~~~ \\n\\nWhich is the better response for the prompt? a or b or tie?\\n\\nAnswer: ', add_special_tokens=False)[\"input_ids\"]\n",
    "    \n",
    "    label_token_id = LABEL_IDS[int(example['label'])]\n",
    "    \n",
    "    # Combine the tokens\n",
    "    input_ids = [tokenizer.bos_token_id] + conversation_a_separator + conversation_a + conversation_b_separator + conversation_b + final_separator + [label_token_id] + [tokenizer.eos_token_id]\n",
    "    attention_mask = [1] * len(input_ids)\n",
    "    labels = [-100] * (len([tokenizer.bos_token_id] + conversation_a_separator + conversation_a + conversation_b_separator + conversation_b + final_separator)) + [label_token_id] + [tokenizer.eos_token_id]\n",
    "    \n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"labels\": labels\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_data(df, tokenizer):\n",
    "    raw_datasets = Dataset.from_pandas(df)\n",
    "    tokenized_datasets = raw_datasets.map(\n",
    "        tokenize, \n",
    "        remove_columns=raw_datasets.column_names,\n",
    "        fn_kwargs={'tokenizer': tokenizer}\n",
    "    )\n",
    "    return tokenized_datasets\n",
    "\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    logits, labels = pred\n",
    "    preds = logits.argmax(axis=-1)\n",
    "    label_tokens_ids = np.array(LABEL_IDS)\n",
    "    index_mapping = {value.item(): idx for idx, value in enumerate(label_tokens_ids)}\n",
    "    labels = labels[np.isin(labels, label_tokens_ids)]\n",
    "    labels = np.array([index_mapping[label.item()] for label in labels])\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    probs = softmax(logits, axis=-1)\n",
    "    log_loss_ = log_loss(labels, probs)\n",
    "    return {'accuracy': acc, 'log_loss': log_loss_}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ultra = pd.read_csv('pseudo-labeling/ultrachat_s42_a0.5.csv')\n",
    "ultra['label'] = ultra[target_columns].idxmax(axis=1) \n",
    "label_encoder = LabelEncoder()\n",
    "ultra['label'] = label_encoder.fit_transform(ultra['label'])\n",
    "ultra = ultra[columns_to_vectorize + ['label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import concatenate_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 57477/57477 [02:47<00:00, 343.12 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 34190/34190 [01:58<00:00, 287.64 examples/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected a list of Dataset objects or a list of IterableDataset objects, but element at position 0 is a dict.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 19\u001b[0m\n\u001b[1;32m     15\u001b[0m train_idx, eval_idx \u001b[38;5;241m=\u001b[39m folds[fold_idx]\n\u001b[1;32m     18\u001b[0m train_ds \u001b[38;5;241m=\u001b[39m ds\u001b[38;5;241m.\u001b[39mselect(train_idx)\n\u001b[0;32m---> 19\u001b[0m train_ds \u001b[38;5;241m=\u001b[39m \u001b[43mconcatenate_datasets\u001b[49m\u001b[43m(\u001b[49m\u001b[43multra_ds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_ds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m eval_ds \u001b[38;5;241m=\u001b[39m ds\u001b[38;5;241m.\u001b[39mselect(eval_idx)\n",
      "File \u001b[0;32m/opt/conda/envs/llama-sft/lib/python3.10/site-packages/datasets/combine.py:201\u001b[0m, in \u001b[0;36mconcatenate_datasets\u001b[0;34m(dsets, info, split, axis)\u001b[0m\n\u001b[1;32m    193\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    194\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected a list of Dataset objects or a list of IterableDataset objects, but element at position \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    195\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis an empty dataset dictionary.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    196\u001b[0m             )\n\u001b[1;32m    197\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    198\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset at position \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m has at least one split: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(dataset)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    199\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease pick one to interleave with the other datasets, for example: dataset[\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(dataset))\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    200\u001b[0m         )\n\u001b[0;32m--> 201\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    202\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected a list of Dataset objects or a list of IterableDataset objects, but element at position \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is a \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(dataset)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    203\u001b[0m     )\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    205\u001b[0m     dataset_type, other_type \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    206\u001b[0m         (Dataset, IterableDataset) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dataset, Dataset) \u001b[38;5;28;01melse\u001b[39;00m (IterableDataset, Dataset)\n\u001b[1;32m    207\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Expected a list of Dataset objects or a list of IterableDataset objects, but element at position 0 is a dict."
     ]
    }
   ],
   "source": [
    "n_splits = 5\n",
    "fold_idx = 0\n",
    "\n",
    "ds = load_data(train, tokenizer)\n",
    "ultra_ds = load_data(ultra, tokenizer)\n",
    "\n",
    "folds = [\n",
    "    (\n",
    "        [i for i in range(len(ds)) if i % n_splits != fold_idx],\n",
    "        [i for i in range(len(ds)) if i % n_splits == fold_idx]\n",
    "    ) \n",
    "    for fold_idx in range(n_splits)\n",
    "]\n",
    "\n",
    "train_idx, eval_idx = folds[fold_idx]\n",
    "\n",
    "\n",
    "train_ds = ds.select(train_idx)\n",
    "train_ds = concatenate_datasets((ultra_ds, train_ds))\n",
    "eval_ds = ds.select(eval_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(80171, 11496)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_ds), len(eval_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|>~~~~~~~~~~ CONVERSATION WITH BOT A ~~~~~~~~~~\n",
      "\n",
      "### User: \"three kids eat three apples in three days, how many apples will nine kids eat in nine days?\"\n",
      "\n",
      "### Bot A Response: \"27 apples\"\n",
      "\n",
      "\n",
      "\n",
      "~~~~~~~~~~ CONVERSATION WITH BOT B ~~~~~~~~~~\n",
      "\n",
      "### User: \"three kids eat three apples in three days, how many apples will nine kids eat in nine days?\"\n",
      "\n",
      "### Bot B Response: \"If three kids eat three apples in three days, each kid eats one apple in three days (since 3 apples \\/ 3 kids = 1 apple per kid).\n",
      "\n",
      "Now, let's find out how many apples each kid would eat in nine days. Since it takes each kid three days to eat one apple, in nine days, each kid would eat three apples (since 9 days \\/ 3 days per apple = 3 apples per kid).\n",
      "\n",
      "Now, we can calculate how many apples nine kids would eat in nine days:\n",
      "\n",
      "9 kids * 3 apples per kid = 27 apples\n",
      "\n",
      "So, nine kids would eat 27 apples in nine days.\"\n",
      "\n",
      "\n",
      "\n",
      "~~~~~~~~~~ \n",
      "\n",
      "Which is the better response for the prompt? a or b or tie?\n",
      "\n",
      "Answer: a<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(train_ds[-1]['input_ids']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Llama3ForSFT(LlamaPreTrainedModel):\n",
    "    _tied_weights_keys = [\"lm_head.weight\"]\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.model = LlamaModel(config)\n",
    "        self.vocab_size = config.vocab_size\n",
    "        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n",
    "        self.post_init()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids= None,\n",
    "        attention_mask= None,\n",
    "        position_ids = None,\n",
    "        past_key_values= None,\n",
    "        inputs_embeds= None,\n",
    "        labels= None,\n",
    "        use_cache= None,\n",
    "        output_attentions= None,\n",
    "        output_hidden_states = None,\n",
    "        return_dict= None,\n",
    "        cache_position = None,\n",
    "    ):\n",
    "        outputs = self.model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            position_ids=position_ids,\n",
    "            past_key_values=past_key_values,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            use_cache=use_cache,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "            cache_position=cache_position,\n",
    "        )\n",
    "        hidden_states = outputs[0]\n",
    "        if self.config.pretraining_tp > 1:\n",
    "            lm_head_slices = self.lm_head.weight.split(self.vocab_size // self.config.pretraining_tp, dim=0)\n",
    "            logits = [F.linear(hidden_states, lm_head_slices[i]) for i in range(self.config.pretraining_tp)]\n",
    "            logits = torch.cat(logits, dim=-1)\n",
    "        else:\n",
    "            logits = self.lm_head(hidden_states)\n",
    "        logits = logits.float()\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            # Shift so that tokens < n predict n\n",
    "            shift_logits = logits[..., :-1, :].contiguous()\n",
    "            shift_labels = labels[..., 1:].contiguous()\n",
    "            # Flatten the tokens\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            shift_logits = shift_logits.view(-1, self.config.vocab_size)\n",
    "            shift_labels = shift_labels.view(-1)\n",
    "            # Enable model parallelism\n",
    "            shift_labels = shift_labels.to(shift_logits.device)\n",
    "\n",
    "            label_tokens_ids = torch.tensor(LABEL_IDS,device=shift_labels.device)\n",
    "            index_mapping = {value.item(): idx for idx, value in enumerate(label_tokens_ids)}\n",
    "            true_labels = shift_labels[torch.isin(shift_labels, label_tokens_ids)]\n",
    "            true_labels = torch.tensor([index_mapping[label.item()] for label in true_labels], device=true_labels.device)\n",
    "            true_logits = shift_logits[torch.isin(shift_labels, label_tokens_ids)][:,label_tokens_ids]\n",
    "            loss = loss_fct(true_logits, true_labels)\n",
    "\n",
    "        return CausalLMOutputWithPast(\n",
    "            loss=loss,\n",
    "            logits=true_logits,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n",
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PeftModelForCausalLM(\n",
      "  (base_model): LoraModel(\n",
      "    (model): Llama3ForSFT(\n",
      "      (model): LlamaModel(\n",
      "        (embed_tokens): Embedding(128256, 4096)\n",
      "        (layers): ModuleList(\n",
      "          (0-31): 32 x LlamaDecoderLayer(\n",
      "            (self_attn): LlamaSdpaAttention(\n",
      "              (q_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=16, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (k_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (v_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "              (rotary_emb): LlamaRotaryEmbedding()\n",
      "            )\n",
      "            (mlp): LlamaMLP(\n",
      "              (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "              (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "              (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "              (act_fn): SiLU()\n",
      "            )\n",
      "            (input_layernorm): LlamaRMSNorm()\n",
      "            (post_attention_layernorm): LlamaRMSNorm()\n",
      "          )\n",
      "        )\n",
      "        (norm): LlamaRMSNorm()\n",
      "      )\n",
      "      (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "trainable params: 9,437,184 || all params: 8,039,698,432 || trainable%: 0.1174\n"
     ]
    }
   ],
   "source": [
    "peft_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias='none',\n",
    "    inference_mode=False,\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    target_modules=['q_proj', 'k_proj', 'v_proj',], \n",
    ")\n",
    "\n",
    "model = Llama3ForSFT.from_pretrained(\n",
    "    model_path, \n",
    "    torch_dtype=torch.float16, \n",
    ")\n",
    "model.config.use_cache = False\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "model = get_peft_model(model, peft_config)\n",
    "print(model)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/llama-sft/lib/python3.10/site-packages/transformers/training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "args = TrainingArguments(\n",
    "    output_dir='output_conversational',\n",
    "    overwrite_output_dir = True,\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    save_strategy = \"steps\",\n",
    "    save_steps=200,\n",
    "    save_total_limit=1,\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=10,\n",
    "    warmup_steps=20,\n",
    "    optim=\"adamw_8bit\",\n",
    "    learning_rate=2e-4,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    gradient_accumulation_steps=2,\n",
    "    num_train_epochs=1,\n",
    "    fp16=True,\n",
    "    metric_for_best_model=\"log_loss\",\n",
    "    greater_is_better = False,\n",
    "    report_to=\"wandb\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/llama-sft/lib/python3.10/site-packages/accelerate/accelerator.py:482: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "/opt/conda/envs/llama-sft/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/opt/conda/envs/llama-sft/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='146' max='10021' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  146/10021 11:10 < 12:46:14, 0.21 it/s, Epoch 0.01/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    args=args,\n",
    "    model=model,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=eval_ds,\n",
    "    data_collator=DataCollatorForSeq2Seq(tokenizer=tokenizer),\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('output_conversational/final_model/tokenizer_config.json',\n",
       " 'output_conversational/final_model/special_tokens_map.json',\n",
       " 'output_conversational/final_model/tokenizer.json')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.model.save_pretrained('output_conversational/final_model')\n",
    "tokenizer.save_pretrained('output_conversational/final_model')"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "llama-sft",
   "name": "common-cu121.m119",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/base-cu121:m119"
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 8346466,
     "sourceId": 66631,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30733,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "LLaMA SFT",
   "language": "python",
   "name": "llama-sft"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
